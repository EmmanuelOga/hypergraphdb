#summary Distributed Dataflow

== Introduction ==

Flow-based programming is a powerful computational model that is becoming ever more relevant  in today's highly concurrent multi-core, distributed systems. It is more powerful and as scalable as something like map-reduce. The premise is very simple: computation is described in terms of black-box processes with well defined inputs and outputs. Each process runs in its own thread, reads inputs from a set of input channels, does something and then writes output to a set of output channels. There is no globally shared data and therefore there are no race conditions possible. Thus, parallelism is inherent to this model and arbitrary computations can be represented with it. There have been attempts in the past to develop the idea as a fundamental programming paradigm (a book, probably easily findable around the internet, that describe the idea in detail is called "Flow-Based Programming" as I remember). More recently, some research on live systems where computation is immediately effective as the programmer edits the program use the same idea. Programming lower-level logic in a flow-based manner can seem cumbersome and inefficient. But for a coarse-grained type of processing, especially in a distributed setting, the paradigm is powerful and convenient.

The basic idea is that a processing node has a bunch of incoming streams of data and bunch of outgoing streams. It continuously receives data from its input and it writes to one or more of its outputs. A simple way to view this is that the incoming streams provide a continuous source of parameters for a function to perform a computation and produce a continuous stream of results. The advantages of the flow-based approach are that parallel asynchronous processing and memory management (disposal of intermediary results that are no longer needed) are obtained for free as inherent to the model. Also, processing based on a flow-based model will scale seamlessly on many CPUs and machines. The disadvantage is that the programming logic may seem unnatural at first and it may prove difficult to live without a global variable namespace. In any case, in our implementation, each processing node has access to an arbitrary global context object provided by the application - this opens the door to working with global, shared state if need be in isolated and carefully controlled cases such as outputting the final results of the computation to a database, or accessing some global, read-only parameters.

The dataflow HyperGraphDB application component offers an implementation of this concept that is distributed across machines. The flow network of processes and channels can be thought of as a directed hypergraph, where each channel is a directed hyperedge connecting all processes writing to it and all processes reading from it. In the distributed version, it is in fact represented as such.

The core, in-process part of the framework, however, is *independent* of HyperGraphDB and can be used standalone. The distributed version uses the HyperGraphDB peer-to-peer framework for communication and also persists the topology in the local database instance of each peer. The intended use is data processing with HyperGraphDB as ultimate storage, but that doesn't have to be the case - processing results can be stored in files, RDBMs or whatever. It is in theory possible not to persist anything and dispense with having HGDB database instances altogether. 

This module was initially developed for a NLP (natural language processing) project called [http://code.google.com/p/disko Disko]. 

== API Overview ==

The API resides as a HyperGraphDB app called `dataflow` alongside the other HyperGraphDBApplications. The main package of the code is as expected:

`org.hypergraphdb.app.dataflow`


The main classes of interest are the following:

|| *Class* || *Description* ||
|| [http://www.hypergraphdb.org/docs/apps/dataflow/org/hypergraphdb/app/dataflow/DataFlowNetwork.html DataFlowNetwork] || Represents a data flow network: essentially the program implemented using this computational model ||
|| [http://www.hypergraphdb.org/docs/apps/dataflow/org/hypergraphdb/app/dataflow/Processor.html Processor] || The interface that a processing node in the network must implement. A processing node takes a bunch of inputs and produces a bunch of outputs. The logic that it implements can be arbitrarily complex and coarse grained, or something as simple as adding two numbers. ||
|| [http://www.hypergraphdb.org/docs/apps/dataflow/org/hypergraphdb/app/dataflow/Channel.html Channel] || Represents a communication channel between processing nodes, or a _pipe_ if you will. A channel can be written to and read from by multiple processing nodes. It is also capable of buffering data that flows through it and blocking writes until that data is being processed. A channel carries a specific type of data and only that type of data. A special value represent EOS (end-of-stream) must be specified when the channel is constructed.  ||
|| [http://www.hypergraphdb.org/docs/apps/dataflow/org/hypergraphdb/app/dataflow/InputPort.html InputPort] || Represents a _read_ connection between a channel and a processing node. A node reads from a channel through an `InputPort`. ||
|| [http://www.hypergraphdb.org/docs/apps/dataflow/org/hypergraphdb/app/dataflow/OutputPort.html OutputPort] || Represents a _write_ connection between a channel and a processing node. A node writes to a channel via an `OutputPort`. ||

To create a network, one needs to implement the processing nodes as implementations of the <code>Processor</code> interface. The <code>AbstractProcessor</code> class can be extended instead to get a default implementation of the <code>getName</code> method. This is convenient when troubleshooting because the processor name because the thread name of the thread in which it's executing, and this makes thread dumps more readable.

Then one needs to connect those processing nodes through communication channels. The connections are established through [http://www.hypergraphdb.org/docs/apps/dataflow/org/hypergraphdb/app/dataflow/Port.html ports] - input ports or output ports. Let's take a look at the <code>Processor.process</code> method:

{{{
public void process(ContextType context, Ports ports) throws InterruptedException
{
   // Context is the global, arbitrary object that the network instance passes 
   // from the application onto all processing nodes. It's application specific and
   // can be anything.

   // The 'ports' parameter represents all input and output connections to channels 
   // in the network. Since each channel has a unique name, the ports are retrieved 
   // by name. Also, since each channel carries potentially a different type of data
   // ports are parameterized by this data type. 

   // To obtain an input port connected to a given input channel, named "inData" and
   // that carries data of type some class A:
   
   InputPort<A> in = ports.getInputPort("inData");

   // to get the next input blocking until there's some available:
   A a = in.take();

   // to check if there's no more input
   if (in.isEOS(a))
     .... do something...maybe simple return 

   // to check if there's data available
   a = in.poll();

   // An input is also an Iteratable so you can read all data in a for loop:
   for (A a : in)
     .... do something with a


   // To obtain an output port:
   OutputPort<B> b = ports.getOutputPort("outData");
   
   // to write to it:
   b.put(new B(a));

   // Any port can be closed at any time.
   in.close();
}
}}}
== Sample Application ==

== Distributed Version ==

TODO -  the API needs some cleaning up and documentation as well as the sample app needs to have a distributed version as a showcase. When that's done, we'll the appropriate description here.